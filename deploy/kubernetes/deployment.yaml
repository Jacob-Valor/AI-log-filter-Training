# =============================================================================
# AI Log Filter - Kubernetes Production Deployment
# =============================================================================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-log-filter
  namespace: security
  labels:
    app: ai-log-filter
    component: classifier
    version: v1.0.0
spec:
  replicas: 6
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 2
      maxUnavailable: 1
  selector:
    matchLabels:
      app: ai-log-filter
      component: classifier
  template:
    metadata:
      labels:
        app: ai-log-filter
        component: classifier
        version: v1.0.0
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
        prometheus.io/path: "/metrics"
    spec:
      serviceAccountName: ai-log-filter

      # Anti-affinity for high availability
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: ai-log-filter
                topologyKey: kubernetes.io/hostname
            - weight: 50
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: ai-log-filter
                topologyKey: topology.kubernetes.io/zone

      # Topology spread for zone distribution
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: ai-log-filter

      containers:
        - name: ai-filter
          image: ai-log-filter:1.0.0
          imagePullPolicy: Always

          ports:
            - name: http
              containerPort: 8000
            - name: metrics
              containerPort: 9090

          env:
            - name: APP_ENV
              value: "production"
            - name: LOG_LEVEL
              value: "INFO"
            - name: MODEL_PATH
              value: "/app/models/production"
            - name: KAFKA_BOOTSTRAP_SERVERS
              valueFrom:
                configMapKeyRef:
                  name: ai-log-filter-config
                  key: kafka.bootstrap_servers
            - name: KAFKA_INPUT_TOPIC
              valueFrom:
                configMapKeyRef:
                  name: ai-log-filter-config
                  key: kafka.input_topic
            - name: QRADAR_HOST
              valueFrom:
                secretKeyRef:
                  name: ai-log-filter-secrets
                  key: qradar.host
            - name: QRADAR_TOKEN
              valueFrom:
                secretKeyRef:
                  name: ai-log-filter-secrets
                  key: qradar.token
            - name: S3_ENDPOINT
              valueFrom:
                configMapKeyRef:
                  name: ai-log-filter-config
                  key: s3.endpoint
            - name: S3_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: ai-log-filter-secrets
                  key: s3.access_key
            - name: S3_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: ai-log-filter-secrets
                  key: s3.secret_key

          resources:
            requests:
              cpu: "2"
              memory: "4Gi"
            limits:
              cpu: "4"
              memory: "8Gi"

          # Liveness probe - is the process alive?
          livenessProbe:
            httpGet:
              path: /health/live
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3

          # Readiness probe - is the service ready for traffic?
          readinessProbe:
            httpGet:
              path: /health/ready
              port: http
            initialDelaySeconds: 45
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3

          # Startup probe - allow time for model loading
          startupProbe:
            httpGet:
              path: /health/ready
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 30 # 5 minutes to start

          volumeMounts:
            - name: models
              mountPath: /app/models
              readOnly: true
            - name: config
              mountPath: /app/configs/production.yaml
              subPath: production.yaml
              readOnly: true
            - name: logs
              mountPath: /var/log/ai-log-filter

          securityContext:
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 1000
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL

      volumes:
        - name: models
          persistentVolumeClaim:
            claimName: ai-log-filter-models
        - name: config
          configMap:
            name: ai-log-filter-config
        - name: logs
          emptyDir: {}

      # Graceful shutdown
      terminationGracePeriodSeconds: 60

---
# =============================================================================
# Horizontal Pod Autoscaler
# =============================================================================
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ai-log-filter
  namespace: security
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ai-log-filter
  minReplicas: 3
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    - type: Pods
      pods:
        metric:
          name: kafka_consumer_lag
        target:
          type: AverageValue
          averageValue: "5000"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Percent
          value: 25
          periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Percent
          value: 100
          periodSeconds: 30
        - type: Pods
          value: 4
          periodSeconds: 30

---
# =============================================================================
# Pod Disruption Budget
# =============================================================================
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: ai-log-filter
  namespace: security
spec:
  minAvailable: 50%
  selector:
    matchLabels:
      app: ai-log-filter

---
# =============================================================================
# Service
# =============================================================================
apiVersion: v1
kind: Service
metadata:
  name: ai-log-filter
  namespace: security
  labels:
    app: ai-log-filter
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8000
      targetPort: http
    - name: metrics
      port: 9090
      targetPort: metrics
  selector:
    app: ai-log-filter

---
# =============================================================================
# Service Monitor (for Prometheus Operator)
# =============================================================================
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: ai-log-filter
  namespace: security
  labels:
    app: ai-log-filter
spec:
  selector:
    matchLabels:
      app: ai-log-filter
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics
  namespaceSelector:
    matchNames:
      - security

---
# =============================================================================
# ConfigMap
# =============================================================================
apiVersion: v1
kind: ConfigMap
metadata:
  name: ai-log-filter-config
  namespace: security
data:
  kafka.bootstrap_servers: "kafka-cluster-kafka-bootstrap.kafka:9092"
  kafka.input_topic: "raw-logs"
  s3.endpoint: "https://s3.us-east-1.amazonaws.com"
  s3.bucket: "security-log-archive"

  # Production config file
  production.yaml: |
    # Embedded from configs/production.yaml

    app:
      name: "AI Log Filter"
      version: "1.0.0"
      env: "production"
      debug: false

    logging:
      level: ${LOG_LEVEL:-INFO}
      format: "json"
      output: "stdout"

    # Lightweight HTTP server for Kubernetes probes.
    health:
      host: "0.0.0.0"
      port: ${HEALTH_PORT:-8000}

    monitoring:
      prometheus:
        enabled: true
        host: "0.0.0.0"
        port: ${PROMETHEUS_PORT:-9090}
        path: "/metrics"

    ingestion:
      kafka:
        bootstrap_servers: ${KAFKA_BOOTSTRAP_SERVERS:-kafka:29092}
        consumer:
          group_id: ${KAFKA_CONSUMER_GROUP:-ai-log-filter-group}
          auto_offset_reset: ${KAFKA_AUTO_OFFSET_RESET:-latest}
          session_timeout_ms: 30000
          heartbeat_interval_ms: 10000
        producer:
          acks: "all"
          retries: 3
          batch_size: 16384
          linger_ms: 10
        topics:
          input: ${KAFKA_INPUT_TOPIC:-raw-logs}
          output: ${KAFKA_OUTPUT_TOPIC:-filtered-logs}

    processing:
      batch_size: ${PROCESSING_BATCH_SIZE:-256}
      max_wait_ms: ${PROCESSING_MAX_WAIT_MS:-100}

    model:
      type: ${MODEL_TYPE:-safe_ensemble}
      path: ${MODEL_PATH:-/app/models/latest}
      timeout_seconds: ${MODEL_TIMEOUT_SECONDS:-5.0}
      max_batch_size: ${MODEL_MAX_BATCH_SIZE:-1000}
      ensemble:
        combination_strategy: "weighted_average"
        weights:
          rule_based: 0.30
          tfidf_xgboost: 0.45
          anomaly_detector: 0.25
      rule_based:
        rules_path: ${RULES_PATH:-configs/rules.yaml}
      compliance:
        enabled: true

    routing:
      qradar:
        host: ${QRADAR_HOST:-qradar.example.com}
        port: ${QRADAR_PORT:-443}
        token: ${QRADAR_TOKEN:-}
        verify_ssl: ${QRADAR_VERIFY_SSL:-true}
        timeout: ${QRADAR_TIMEOUT:-30}
      cold_storage:
        type: ${COLD_STORAGE_TYPE:-local}
        bucket: ${COLD_STORAGE_BUCKET:-log-archive}
        batch_size: ${COLD_STORAGE_BATCH_SIZE:-10000}
        compression: ${COLD_STORAGE_COMPRESSION:-gzip}
      summary:
        aggregation_window_seconds: ${SUMMARY_WINDOW_SECONDS:-3600}
      rules:
        critical:
          method: immediate
          destinations: [qradar, cold_storage]
          batch_size: 1
        suspicious:
          method: queue
          destinations: [qradar, cold_storage]
          batch_size: 100
        routine:
          method: queue
          destinations: [cold_storage]
          batch_size: 500
        noise:
          method: queue
          destinations: [summary, cold_storage]
          batch_size: 1000
